# -*- coding: utf-8 -*-
"""Emotions_detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXG0fYO4eQ1k2e-8ZMDnAQ4XpeWT34kH
"""

from google.colab import files
uploaded = files.upload()

!pip uninstall -y scipy gensim
!pip install numpy==1.26.4
!pip install scipy==1.13.1 gensim==4.3.3 xgboost optuna wordcloud textblob seaborn

import nltk
nltk.download('punkt')
nltk.download('stopwords')

import numpy
import scipy
import gensim
import xgboost

print(" numpy:", numpy.__version__)
print(" scipy:", scipy.__version__)
print(" gensim:", gensim.__version__)
print(" xgboost:", xgboost.__version__)

import pandas as pd

df = pd.read_csv("text.csv")
df.head()

import gensim.downloader as api
from gensim.models import Word2Vec

import pandas as pd
import numpy as np

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, ImageColorGenerator
from PIL import Image  # Image processing

# processing
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords  # Common stop words
from nltk.stem import PorterStemmer  # Stemming words
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# text analysis
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer


from sklearn.utils import resample


import optuna
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV  # Hyperparameter tuning
from sklearn.model_selection import StratifiedKFold

# Word embeddings
import gensim.downloader as api
from gensim.models import Word2Vec

# Machine learning models
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC

from sklearn import metrics

from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, SpatialDropout1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


import warnings
warnings.filterwarnings('ignore')
from collections import Counter

## About the dataset

df.info()

df.describe()

import matplotlib.pyplot as plt
import seaborn as sns

def classes_plot(df):
  fig , ax = plt.subplots(figsize=(6,4))
  sns.countplot(x='label', data=df, color='cyan')
  plt.title("sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5)")
  plt.show()

classes_plot(df)

"""# **As we can see the dataset is not balanced, so we use undersample technique to make the dataset balanced according to the smallest class (surprise)**"""

# Splitting the dataset to 3 different datasets as mentioned above
# Starting with the simple one - keeping the original dataset
original_df = df

from sklearn.utils import resample

# Now we will apply undersampling to the majority class to match class 5

# Separate the dataset by class
df_0 = df[df['label'] == 0]
df_1 = df[df['label'] == 1]
df_2 = df[df['label'] == 2]
df_3 = df[df['label'] == 3]
df_4 = df[df['label'] == 4]
df_5 = df[df['label'] == 5]

# Saving the amount of data in label 5
n_samples_label_5 = round(len(df_5) / 2)

# downsample the majority classes to n_samples_label_5 samples
df_0_downsampled = resample(df_0, replace=False, n_samples=n_samples_label_5, random_state=42)
df_1_downsampled = resample(df_1, replace=False, n_samples=n_samples_label_5, random_state=42)
df_2_downsampled = resample(df_2, replace=False, n_samples=n_samples_label_5, random_state=42)
df_3_downsampled = resample(df_3, replace=False, n_samples=n_samples_label_5, random_state=42)
df_4_downsampled = resample(df_4, replace=False, n_samples=n_samples_label_5, random_state=42)
df_5_downsampled = resample(df_5, replace=False, n_samples=n_samples_label_5, random_state=42)

df_downsampled = pd.concat([df_1_downsampled, df_0_downsampled, df_3_downsampled, df_4_downsampled, df_2_downsampled, df_5_downsampled])

df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"Number of data for each class is: {n_samples_label_5}")
classes_plot(df_downsampled)
df_downsampled.head()

def extract_href_sentence_lengths(df, text_column='text'):
    """
    Extracts the lengths of sentences that start with the words 'href', 'http' and 'www' in a given DataFrame column.

    Parameters:
    - df: DataFrame containing the data
    - text_column: Name of the column containing text data (default: 'text')

    Returns:
    - Lists of sentence lengths that start with 'href', 'http' and 'www'
    """
    href_sentence_lengths = []
    http_sentence_lengths = []
    www_sentence_lengths = []
    for text in df[text_column]:
        sentences = text.split(' ')
        for sentence in sentences:
            if sentence.lower().startswith('href'):
                href_sentence_lengths.append(len(text))
            elif sentence.lower().startswith('http'):
                http_sentence_lengths.append(len(text))
            elif sentence.lower().startswith('www'):
                www_sentence_lengths.append(len(text))
    return href_sentence_lengths, http_sentence_lengths, www_sentence_lengths

def plot_text_length_distribution(df, df_name='DataFrame', text_column='text', label_column='label'):
    """
    Plots the distribution of text lengths by class.

    param: df: DataFrame containing the data
    param: df_name: Name of the DataFrame (default: 'DataFrame')
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    """
    df['text_length'] = df[text_column].apply(len)
    plt.figure(figsize=(10, 4))
    sns.histplot(data=df, x='text_length', hue=label_column, multiple='stack', bins=30)
    plt.title(f"Distribution of Text Lengths by Class for '{df_name}' dataset")
    plt.xlabel('Text Length')
    plt.ylabel('Frequency')
    plt.show()

def plot_word_clouds(df, df_name='DataFrame', text_column='text', label_column='label'):
    """
    Generates word clouds for each class.

    param: df: DataFrame containing the data
    param: df_name: Name of the DataFrame (default: 'DataFrame')
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    """
    def plot_word_cloud(text, label):
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(9, 4))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f"Word Cloud for Label {label} in '{df_name}' dataset")
        plt.axis('off')
        plt.show()

    for label in df[label_column].unique():
        text = ' '.join(df[df[label_column] == label][text_column])
        plot_word_cloud(text, label)

# function to plot the most common words for each class
def plot_top_n_words(df, df_name='DataFrame', text_column='text', label_column='label', n=10):
    """
    Plots the top N most common words for each class.

    param: df: DataFrame containing the data
    param: df_name: Name of the DataFrame (default: 'DataFrame')
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    param: n: Number of top words to plot (default: 10)
    """
    def plot_words(text, label, n):
        vec = CountVectorizer().fit(text)
        bag_of_words = vec.transform(text)
        sum_words = bag_of_words.sum(axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]
        words, freqs = zip(*words_freq)
        plt.figure(figsize=(10, 4))
        sns.barplot(x=freqs, y=words)
        plt.title(f"Top {n} Words for Label {label} in '{df_name}' dataset")
        plt.xlabel('Frequency')
        plt.ylabel('Word')
        plt.show()

    for label in df[label_column].unique():
        text = df[df[label_column] == label][text_column]
        plot_words(text, label, n)

# function to plot the most common bigrams and trigrams for each class
def plot_ngrams(df, df_name='DataFrame', text_column='text', label_column='label', ngram_range=(2, 2), n=10):
    """
    Plots the most common n-grams (bigrams or trigrams) for each class.

    param: df: DataFrame containing the data
    param: df_name: Name of the DataFrame (default: 'DataFrame')
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    param: ngram_range: Tuple specifying the n-gram range (default: (2, 2))
    param: n: Number of top n-grams to plot (default: 10)
    """
    def plot_ngrams_for_label(text, label, ngram_range, n):
        vec = CountVectorizer(ngram_range=ngram_range).fit(text)
        bag_of_words = vec.transform(text)
        sum_words = bag_of_words.sum(axis=0)
        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]
        words, freqs = zip(*words_freq)
        plt.figure(figsize=(10, 6))
        sns.barplot(x=freqs, y=words)
        plt.title(f"Top {n} {'-'.join(map(str, ngram_range))}grams for Label {label} in '{df_name}' dataset")
        # plt.title(f'Top {n} {"-".join(map(str, ngram_range))}grams for Label {label}')
        plt.xlabel('Frequency')
        plt.ylabel('N-gram')
        plt.show()

    for label in df[label_column].unique():
        text = df[df[label_column] == label][text_column]
        plot_ngrams_for_label(text, label, ngram_range, n)

# function to plot the sentiment polarity distribution by class
def plot_sentiment_distribution(df, df_name='DataFrame', text_column='text', label_column='label'):
    """
    Plots the distribution of sentiment polarity scores by class.

    param: df: DataFrame containing the data
    param: df_name: Name of the DataFrame (default: 'DataFrame')
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    """
    df['sentiment'] = df[text_column].apply(lambda x: TextBlob(x).sentiment.polarity)
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=label_column, y='sentiment', data=df)
    plt.title(f"Sentiment Polarity Distribution by Class for '{df_name}' daaset")
    plt.xlabel('Label')
    plt.ylabel('Sentiment Polarity')
    plt.show()

# extract lengths of sentences starting with 'href' and 'http' and 'www'
href_sentence_lengths, http_sentence_lengths, www_sentence_lengths = extract_href_sentence_lengths(df)


plt.figure(figsize=(12, 6))
plt.hist(href_sentence_lengths, bins=20, alpha=0.5, label='href')
plt.hist(http_sentence_lengths, bins=20, alpha=0.5, label='http')
plt.hist(www_sentence_lengths, bins=20, alpha=0.5, label='www')
plt.xlabel('Sentence Length')
plt.ylabel('Frequency')
plt.title('Distribution of Sentence Lengths')
plt.legend()
plt.show()
print(f"Number of sentences starting with 'href': {len(href_sentence_lengths)}")
print(f"Number of sentences starting with 'http': {len(http_sentence_lengths)}")
print(f"Number of sentences starting with 'www': {len(www_sentence_lengths)}")

# Plotting te text length for both original_df and df_downsampled to see the difference
# plot_text_length_distribution(df=original_df, df_name='original_df')
plot_text_length_distribution(df_downsampled, df_name='df_downsmapled')



from wordcloud import WordCloud

plot_word_clouds(df=df_downsampled, df_name='df_downsmapled')

from sklearn.feature_extraction.text import CountVectorizer

plot_top_n_words(df=df_downsampled, df_name='df_downsmapled')

plot_ngrams(df=df_downsampled,  df_name='df_downsmapled', ngram_range=(3, 3))  # For trigrams

from textblob import TextBlob

plot_sentiment_distribution(df_downsampled)

fig, axes = plt.subplots(1,2, figsize = (20,5))
sns.distplot(df_downsampled['text_length'], hist=True, kde=True,
             color = 'cyan',
             hist_kws={'edgecolor':'black'},
             kde_kws={'color': "b", 'linewidth': 5},
             ax = axes[0])

sns.distplot(df_downsampled['sentiment'], hist=True, kde=True,
             color = 'cyan',
             hist_kws={'edgecolor':'black'},
             kde_kws={'color': "b", 'linewidth': 5},
             ax = axes[1])

"""**Pre-proccessing in the text phase**

In the next few codes (functions), we will apply several techniques such as: Tokenization, Removing stop words, Stemming, Vectorization, and Embedding.

This applied to make the dataset simpler by splitting the words from each other, removing unneccessary words (that don't contribute to the whole sentence), stemming to convert the words to their base form, vectorization
"""

def tokenize_text(df, text_column='text'):
    """
    Tokenizes the text data.

    param: df: DataFrame containing the data
    param: text_column: Name of the column containing text data (default: 'text')
    return: DataFrame with tokenized text
    type: (DataFrame, str) -> DataFrame
    """
    df['tokenized_text'] = df[text_column].apply(word_tokenize)
    return df


def remove_stop_words(df, text_column='tokenized_text'):
    """
    Removes stop words from the tokenized text.

    param: df: DataFrame containing the data
    param: text_column: Name of the column containing tokenized text data (default: 'tokenized_text')
    return: DataFrame with stop words removed
    type: (DataFrame, str) -> DataFrame
    """
    custom_stopwords = {"i","im","like", "feel", "feeling", "my", "the", "to", "still"
                        "for","know","littl","think","time","thing","would","go",
                        "really","feel","am","so","get", "one", "to", "and", "at", "can",
                        "day","way", "make", "me", "want", "could" , "would", "tri", "u", "href", "http", "www", "com", "https"
                        }
    stop_words = set(stopwords.words('english'))
    stop_words.update(custom_stopwords)
    df['text_no_stopwords'] = df[text_column].apply(lambda x: [word for word in x if word.lower() not in stop_words])
    return df

def apply_stemming(df, text_column='text_no_stopwords'):
    """
    Applies stemming to the text data.

    param: df: DataFrame containing the data
    param: text_column: Name of the column containing text data without stop words (default: 'text_no_stopwords')
    return: DataFrame with stemmed text
    type: (DataFrame, str) -> DataFrame
    """
    stemmer = PorterStemmer()
    df['stemmed_text'] = df[text_column].apply(lambda x: [stemmer.stem(word) for word in x])
    return df

def load_glove_vectors(glove_file):
    """
    Loads GloVe vectors from a file.

    param: glove_file: Path to the GloVe file
    return: Dictionary of word to vector mappings
    type: str -> dict
    """
    glove_model = {}
    with open(glove_file, 'r', encoding='utf8') as f:
        for line in f:
            split_line = line.split()
            word = split_line[0]
            embedding = np.array(split_line[1:], dtype=np.float64)
            glove_model[word] = embedding
    return glove_model

def get_glove_embeddings(df, glove_model, text_column='stemmed_text'):
    """
    Gets word embeddings using GloVe.

    param: df: DataFrame containing the data
    param: glove_model: Dictionary of GloVe vectors
    param: text_column: Name of the column containing stemmed text data (default: 'stemmed_text')
    return: DataFrame with word embeddings
    type: (DataFrame, dict, str) -> DataFrame
    """
    def embed_text(tokens):
        embedding_dim = len(next(iter(glove_model.values())))
        embeddings = [glove_model[word] for word in tokens if word in glove_model]
        if embeddings:
            return np.mean(embeddings, axis=0)
        else:
            return np.zeros(embedding_dim)

    df['text_embedding'] = df[text_column].apply(embed_text)
    return df

def preprocess_text_data(df, glove_model, text_column='text', label_column='label'):
    """
    Preprocesses text data through tokenization, stop word removal, stemming, and embedding.

    param: df: DataFrame containing the data
    param: glove_model: Dictionary of GloVe vectors
    param: text_column: Name of the column containing text data (default: 'text')
    param: label_column: Name of the column containing label data (default: 'label')
    return: Preprocessed DataFrame with embeddings
    type: (DataFrame, dict, str, str) -> DataFrame
    """
    df = tokenize_text(df, text_column)
    df = remove_stop_words(df)
    df = apply_stemming(df)
    df = get_glove_embeddings(df, glove_model)
    return df

from google.colab import drive
drive.mount('/content/drive')

# Load GloVe model
glove_file = "drive/MyDrive/kaggle_files/glove.6B.100d.txt"  # Path to the GloVe file
glove_model = load_glove_vectors(glove_file)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

df_downsampled_preprocessed = preprocess_text_data(df_downsampled, glove_model)
print(df_downsampled_preprocessed.head())

df_downsampled_preprocessed.head()

df_downsampled_preprocessed['sentence_stemmed_text'] = df_downsampled_preprocessed['stemmed_text'].apply(lambda x: ','.join(x))

df_downsampled_preprocessed.head()

plot_top_n_words(df=df_downsampled_preprocessed, df_name='df_downsampled_preprocessed', text_column='sentence_stemmed_text')

plt.subplots(figsize=(12,5))
df = df_downsampled_preprocessed.drop(['text', 'tokenized_text', 'text_no_stopwords', 'stemmed_text', 'text_embedding', 'sentence_stemmed_text'], axis=1)
corr = df.corr()
sns.heatmap(corr,annot = True, cmap=sns.diverging_palette(220, 10, as_cmap=True))

"""# Splitting data"""

def split_data(df, test_size=0.2, random_state=42):
  """
  Splits the dataset into training and testing sets.

  param: df: DataFrame containing the data
  param: test_size: Size of the testing set (default: 0.2)
  param: random_state: Random seed for reproducibility (default: 42)

  return: X_train, X_test, y_train, y_test
  """
  # Extract features and target
  # text_length = df['text_length'].values.reshape(-1, 1) tried to omit this column and found out that the results better without this column
  sentiment = df['sentiment'].values.reshape(-1, 1)
  X_embeddings = np.vstack(df['text_embedding'].values)

  X = np.hstack((sentiment, X_embeddings))

  # Extract target
  y = df['label'].values

  # split to train and test
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

  return X_train, X_test, y_train, y_test

# X_train_original, X_test_original, y_train_original, y_test_original = split_data(df=original_df, test_size=0.3)
X_train_downsampled, X_test_downsampled, y_train_downsampled, y_test_downsampled = split_data(df_downsampled_preprocessed)
print(f"Train set length for df_downsampled dataset is {X_train_downsampled.shape[0]}")
print(f"Test set length for df_downsampled dataset is {X_test_downsampled.shape[0]}")

"""# Training and tuning models"""

from sklearn.model_selection import train_test_split

X_knn, _, y_knn, _ = train_test_split(
    X_train_downsampled, y_train_downsampled,
    train_size=0.2, stratify=y_train_downsampled, random_state=42
)

def tune_hyperparameters(model_class, param_distributions, X, y, n_trials=20):

    from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score
    import optuna, time

    X_small, _, y_small, _ = train_test_split(X, y, train_size=0.3, stratify=y, random_state=42)

    def objective(trial):
        params = {key: trial._suggest(key, dist) for key, dist in param_distributions.items()}
        model = model_class(**params)
        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        start = time.time()
        score = cross_val_score(model, X_small, y_small, cv=cv, scoring='accuracy').mean()
        print(f"Trial {trial.number} - time: {time.time() - start:.2f}s - score: {score:.4f} - params: {params}")
        return score

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials)

    return study.best_params, study.best_value

# define hyperparameter for KNeighborsClassifier
param_distributions_knn = {
    'n_neighbors': optuna.distributions.IntDistribution(1, 50),
    'weights': optuna.distributions.CategoricalDistribution(['uniform', 'distance']),
    'algorithm': optuna.distributions.CategoricalDistribution(['auto', 'ball_tree', 'kd_tree'])
}

best_params_knn, best_score_knn = tune_hyperparameters(KNeighborsClassifier, param_distributions_knn, X_train_downsampled, y_train_downsampled, n_trials=15)

print('Best hyperparameters for KNN: ', best_params_knn)
print('Best cross-validation score for KNN: ', best_score_knn)

X_small, _, y_small, _ = train_test_split(X_train_downsampled, y_train_downsampled, train_size=0.1, stratify=y_train_downsampled, random_state=42)

# define hyperparameter space for RandomForestClassifier
param_distributions_rf = {
    'n_estimators': optuna.distributions.IntDistribution(30, 300, 10),
    'max_features': optuna.distributions.CategoricalDistribution(['sqrt', 'log2']),
    'max_depth': optuna.distributions.IntDistribution(2, 20),
    'min_samples_split': optuna.distributions.IntDistribution(2, 14),
    "min_samples_leaf": optuna.distributions.IntDistribution(1, 10)
}

best_params_rf, best_score_rf = tune_hyperparameters(RandomForestClassifier, param_distributions_rf, X_small, y_small, n_trials=15)

print('Best hyperparameters for Random Forest: ', best_params_rf)
print('Best cross-validation score for Random Forest: ', best_score_rf)

# define hyperparameter for LinearSVC
param_distributions_linear_svc = {
    'C': optuna.distributions.LogUniformDistribution(0.1, 10)
}

best_params_linear_svc, best_score_linear_svc = tune_hyperparameters(LinearSVC, param_distributions_linear_svc, X_small, y_small, n_trials=15)

print('Best hyperparameters for LinearSVC: ', best_params_linear_svc)
print('Best cross-validation score for LinearSVC: ', best_score_linear_svc)

import optuna, time
# define hyperparameter for AdaBoost
param_distributions_ada = {
    'n_estimators': optuna.distributions.IntDistribution(50, 200),
    'learning_rate': optuna.distributions.LogUniformDistribution(1e-1, 1.0)
}

best_params_ada, best_score_ada = tune_hyperparameters(AdaBoostClassifier, param_distributions_ada, X_small, y_small, n_trials=15)

print('Best hyperparameters for AdaBoost: ', best_params_ada)
print('Best cross-validation score for AdaBoost: ', best_score_ada)

# define hyperparameter for XGBoost
param_distributions_xgb = {
    'n_estimators': optuna.distributions.IntDistribution(50, 200),
    'max_depth': optuna.distributions.IntDistribution(3, 10),
    'learning_rate': optuna.distributions.LogUniformDistribution(1e-3, 1e-1),
    'subsample': optuna.distributions.FloatDistribution(0.5, 1.0),
    'colsample_bytree': optuna.distributions.FloatDistribution(0.5, 1.0)
}

best_params_xgb, best_score_xgb = tune_hyperparameters(XGBClassifier, param_distributions_xgb, X_small, y_small, n_trials=15)

print('Best hyperparameters for XGBoost: ', best_params_xgb)
print('Best cross-validation score for XGBoost: ', best_score_xgb)

"""**Comparison with Dummy** **models**"""

train_dummy_scores = []
test_dummy_scores = []
strategies = ['most_frequent', 'stratified', 'uniform']

for i in strategies:
    clf_dummy = DummyClassifier(strategy = i, random_state=42)
    clf_dummy.fit(X_train_downsampled, y_train_downsampled)
    y_pred_dummy_train = clf_dummy.predict(X_train_downsampled)
    y_pred_dummy_test = clf_dummy.predict(X_test_downsampled)
    train_accuracy = metrics.accuracy_score(y_true = y_train_downsampled, y_pred = y_pred_dummy_train)
    test_accuracy = metrics.accuracy_score(y_true = y_test_downsampled, y_pred = y_pred_dummy_test)
    train_dummy_scores.append(train_accuracy)
    test_dummy_scores.append(test_accuracy)

print(train_dummy_scores)
print(test_dummy_scores)

f, axes = plt.subplots(1,2, figsize = (15,5))
sns.stripplot(x=strategies, y=train_dummy_scores,size = 10, ax = axes[0])
axes[0].set_title("The accuracies of dummy model for train set")
sns.stripplot(x=strategies, y=test_dummy_scores,size = 10, ax = axes[1])
axes[1].set_title("The accuracies of dummy model for test set")
plt.show()

"""# **Models evaluation using different metrics**"""

knn = KNeighborsClassifier(n_neighbors = 22, weights = 'distance', algorithm = 'ball_tree')
randomForest = RandomForestClassifier(n_estimators=119, max_features='sqrt', max_depth=15, min_samples_split=5, min_samples_leaf=10)
svc = LinearSVC(C=0.13022969239595297)
xgb = XGBClassifier(reg_alpha=0.5, gamma=0.1, n_estimators=95, max_depth=8, learning_rate=0.04989968022036783, subsample=0.6713114776225965, colsample_bytree=0.5096762376471592)
abc = AdaBoostClassifier(n_estimators=160, learning_rate=0.30007391716763326)
classifiers = [knn, svc, xgb, abc, randomForest]
Clf_names = ["knn", "linearSvc", "xgb", "abc", "randomForest"]

tune_train_scores = []
tune_test_scores = []
count = 0
for i in classifiers:
  i.fit(X_train_downsampled, y_train_downsampled)
  y_pred_train_tune = i.predict(X_train_downsampled)
  y_pred_test_tune = i.predict(X_test_downsampled)
  print(f"The train Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_train_downsampled, y_pred = y_pred_train_tune))
  print(f"The test Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_test_downsampled, y_pred = y_pred_test_tune))
  print()
  tune_train_scores.append(metrics.accuracy_score(y_true = y_train_downsampled, y_pred = y_pred_train_tune))
  tune_test_scores.append(metrics.accuracy_score(y_true = y_test_downsampled, y_pred = y_pred_test_tune))
  count = count+1

count = 0
for classifier in classifiers:
  prediction = classifier.predict(X_test_downsampled)
  print(f"Classification report for {Clf_names[count]} model is:", classification_report(y_test_downsampled, prediction))
  count = count+1

count = 0
for classifier in classifiers:
  if i == svc:
    count = count+1
    continue
  prediction = classifier.predict(X_test_downsampled)
  cm = confusion_matrix(y_test_downsampled, prediction)
  plt.figure(figsize=(4, 2))
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
  plt.xlabel('Predicted labels')
  plt.ylabel('True labels')
  plt.title(f"Confusion Matrix for {Clf_names[count]} model")
  plt.show()
  count = count+1

"""# Implement CNN using self embedding"""

vocab_size = 10000
embedding_dim = 100
max_length = 100
trunc_type = 'post'
padding_type = 'post'

# tokenize and pad
tokenizer = Tokenizer(num_words=vocab_size, oov_token='')
tokenizer.fit_on_texts(df_downsampled['text'])
sequences = tokenizer.texts_to_sequences(df_downsampled['text'])
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# split data
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df_downsampled['label'], test_size=0.2, random_state=42)


model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(128, activation='relu'),
    Dense(6, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# train the model
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=2)

# evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_acc}')

"""# Comparing with existing methods (people in kaggle that also tried this dataset)"""

emotion_to_sentiment_3labels = {
    0: 0,  # sadness -> negative sentiment
    1: 1,  # joy/love -> neutral sentiment
    2: 1,
    3: 0,  # anger -> negative sentiment
    4: 2,  # fear/surprise -> positive sentiment
    5: 2   # fear/surprise -> positive sentiment
}

df_compare = df_downsampled_preprocessed.copy()
df_compare['label'] = df_compare['label'].map(emotion_to_sentiment_3labels)

df_compare.head()

X_train_comapre, X_test_compare, y_train_compare, y_test_compare = split_data(df_compare)

tune_train_scores = []
tune_test_scores = []
count = 0
for i in classifiers:
  i.fit(X_train_comapre, y_train_compare)
  y_pred_train_tune = i.predict(X_train_comapre)
  y_pred_test_tune = i.predict(X_test_compare)
  print(f"The train Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_train_compare, y_pred = y_pred_train_tune))
  print(f"The test Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_test_compare, y_pred = y_pred_test_tune))
  print()
  tune_train_scores.append(metrics.accuracy_score(y_true = y_train_compare, y_pred = y_pred_train_tune))
  tune_test_scores.append(metrics.accuracy_score(y_true = y_test_compare, y_pred = y_pred_test_tune))
  count = count+1

"""# Trying different techniques for embedding"""

stemer = PorterStemmer()
stp_words = stopwords.words("english")
def preprocess(text):
    preprocessed_txt = ""
    tokens = word_tokenize(text)
    for token in tokens:
        if token not in stp_words :
            lower_token = token.lower()
            stemed_token = stemer.stem(lower_token)
            preprocessed_txt += stemed_token + " "
    else:
        preprocessed_txt = preprocessed_txt[:-1]
    return preprocessed_txt

# now we will apply undersampling to the majority class to match class 5

# Separate the dataset by class
df_0 = original_df[original_df['label'] == 0]
df_1 = original_df[original_df['label'] == 1]
df_2 = original_df[original_df['label'] == 2]
df_3 = original_df[original_df['label'] == 3]
df_4 = original_df[original_df['label'] == 4]
df_5 = original_df[original_df['label'] == 5]

# saving the amount of data in label 5
n_samples_label_5 = round(len(df_5) / 2)

df_0_downsampled = resample(df_0, replace=False, n_samples=n_samples_label_5, random_state=42)
df_1_downsampled = resample(df_1, replace=False, n_samples=n_samples_label_5, random_state=42)
df_2_downsampled = resample(df_2, replace=False, n_samples=n_samples_label_5, random_state=42)
df_3_downsampled = resample(df_3, replace=False, n_samples=n_samples_label_5, random_state=42)
df_4_downsampled = resample(df_4, replace=False, n_samples=n_samples_label_5, random_state=42)
df_5_downsampled = resample(df_5, replace=False, n_samples=n_samples_label_5, random_state=42)

df_downsampled_second = pd.concat([df_1_downsampled, df_0_downsampled, df_3_downsampled, df_4_downsampled, df_2_downsampled, df_5_downsampled])

df_downsampled_second = df_downsampled_second.sample(frac=1, random_state=42).reset_index(drop=True)

print(f"Number of data for each class is: {n_samples_label_5}")
classes_plot(df_downsampled_second)
df_downsampled_second.head()

df_downsampled_second.head()

df_downsampled_second['text'] = df_downsampled_second['text'].apply(preprocess)

count_vectorizer = CountVectorizer(binary=True, stop_words='english')
X = count_vectorizer.fit_transform(df_downsampled_second['text'])

features = count_vectorizer.get_feature_names_out()

y = df_downsampled_second['label']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

knn = KNeighborsClassifier(n_neighbors = 22, weights = 'distance', algorithm = 'ball_tree')
randomForest = RandomForestClassifier(n_estimators=119, max_features='sqrt', max_depth=15, min_samples_split=5, min_samples_leaf=10)
svc = LinearSVC(C=0.13022969239595297)
xgb = XGBClassifier(reg_lambda=1.0, reg_alpha=0.5, gamma=0.1, n_estimators=95, max_depth=8, learning_rate=0.04989968022036783, subsample=0.6713114776225965, colsample_bytree=0.5096762376471592)
abc = AdaBoostClassifier(n_estimators=160, learning_rate=0.30007391716763326)
classifiers = [knn, svc, xgb, abc, randomForest]
Clf_names = ["knn", "linearSvc", "xgb", "abc", "randomForest"]

tune_train_scores = []
tune_test_scores = []
count = 0
for i in classifiers:
  i.fit(X_train, y_train)
  y_pred_train_tune = i.predict(X_train)
  y_pred_test_tune = i.predict(X_test)
  print(f"The train Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_train, y_pred = y_pred_train_tune))
  print(f"The test Accuracy for {Clf_names[count]} model is:", metrics.accuracy_score(y_true = y_test, y_pred = y_pred_test_tune))
  print()
  tune_train_scores.append(metrics.accuracy_score(y_true = y_train, y_pred = y_pred_train_tune))
  tune_test_scores.append(metrics.accuracy_score(y_true = y_test, y_pred = y_pred_test_tune))
  count = count+1

"""# Tranformer work"""

