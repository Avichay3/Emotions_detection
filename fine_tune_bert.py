# -*- coding: utf-8 -*-
"""Fine_tune_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2ovy47HwKdKMksyag0Y8f9udLXePagI
"""

!pip install -U transformers

!pip install -U transformers --force-reinstall

from google.colab import files
uploaded = files.upload()

import pandas as pd
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from torch.utils.data import Dataset

filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)
df.head()

from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from torch.utils.data import Dataset
import torch
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ðŸ’» Using device:", device)

os.environ["WANDB_DISABLED"] = "true"

df_small = df.sample(n=2000, random_state=42).reset_index(drop=True)

label_encoder = LabelEncoder()
df_small["label_enc"] = label_encoder.fit_transform(df_small["label"])

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df_small["text"].tolist(), df_small["label_enc"].tolist(), test_size=0.2, random_state=42
)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)

class EmotionDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        return {
            **{k: torch.tensor(v[idx]) for k, v in self.encodings.items()},
            "labels": torch.tensor(self.labels[idx])
        }
    def __len__(self):
        return len(self.labels)

train_dataset = EmotionDataset(train_encodings, train_labels)
test_dataset = EmotionDataset(test_encodings, test_labels)

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(label_encoder.classes_)
).to(device)

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=5,
    save_strategy="no"
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), dim=1)
    return {"accuracy": accuracy_score(labels, preds)}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

eval_result = trainer.evaluate()
print("ðŸ“Š Fine-tuned BERT Accuracy:", round(eval_result["eval_accuracy"] * 100, 2), "%")

preds = trainer.predict(test_dataset)
y_pred = preds.predictions.argmax(axis=1)
y_true = preds.label_ids

from sklearn.metrics import accuracy_score
print("Calculated Accuracy:", accuracy_score(y_true, y_pred))

